# LLM Provider Configuration
LLM_PROVIDER=openai                  # LLM provider: "openai" or "gemini" (default: openai)

# OpenAI API Configuration (default provider)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o-mini             # OpenAI model to use (default: gpt-4o-mini)
OPENAI_TIMEOUT=60                    # Request timeout in seconds (default: 60)
OPENAI_TEMPERATURE=0.1               # Temperature for LLM generation 0.0-2.0 (default: 0.1)

# Gemini API Configuration (for rollback)
GEMINI_API_KEY=your_gemini_api_key_here
GEMINI_MODEL=gemini-2.5-flash        # Gemini model to use (default: gemini-2.5-flash)
GEMINI_TIMEOUT=60                    # Request timeout in seconds (default: 60)
GEMINI_TEMPERATURE=0.1               # Temperature for LLM generation 0.0-2.0 (default: 0.1)

# Optional: Temporary Directory for Diagram Files
TMP_DIR=/tmp/diagrams

# Optional: Vertex AI Configuration (if using Vertex AI instead of Gemini Developer API)
USE_VERTEX_AI=false
GOOGLE_CLOUD_PROJECT=your_gcp_project_id
GOOGLE_CLOUD_LOCATION=us-central1

# Optional: Application Features
USE_CRITIQUE_GENERATION=true        # Enable critique-enhanced diagram generation (default: true)
CRITIQUE_MAX_ATTEMPTS=3              # Max critique attempts for better quality (1-5, default: 3)
MOCK_LLM=false                       # Use mock LLM for testing without API calls (default: false)

# Optional: Network Retry Configuration
ANALYSIS_MAX_ATTEMPTS=2              # Retries for analysis LLM calls (default: 2)
ADJUST_MAX_ATTEMPTS=2                # Retries for adjust LLM calls (default: 2)
RETRY_BACKOFF_BASE=0.5               # Exponential backoff base in seconds (default: 0.5)
RETRY_BACKOFF_MAX=4.0                # Backoff cap in seconds (default: 4.0)
RETRY_JITTER=0.25                    # Jitter in seconds for retry timing (default: 0.25)
